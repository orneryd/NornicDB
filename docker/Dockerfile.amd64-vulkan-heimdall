# NornicDB AMD64 Vulkan + Heimdall
# GPU-accelerated build using Vulkan with Heimdall AI assistant
# Works with any Vulkan-capable GPU: NVIDIA, AMD, Intel
# Includes llama.cpp CPU inference for Heimdall (no CUDA required)
#
# Build:  docker build -f docker/Dockerfile.amd64-vulkan-heimdall -t nornicdb-amd64-vulkan-heimdall .
# With models embedded:
#         docker build -f docker/Dockerfile.amd64-vulkan-heimdall --build-arg EMBED_MODEL=true -t nornicdb-amd64-vulkan-heimdall-bge .
# Headless (no UI):
#         docker build -f docker/Dockerfile.amd64-vulkan-heimdall --build-arg HEADLESS=true -t nornicdb-amd64-vulkan-heimdall-headless .

# =============================================================================
# Stage 1: UI (skipped in headless mode)
# =============================================================================
FROM node:20-alpine AS ui
ARG HEADLESS=false
WORKDIR /ui
COPY ui/package*.json ./
RUN if [ "$HEADLESS" != "true" ]; then \
      npm ci 2>/dev/null || npm install --legacy-peer-deps; \
    fi
COPY ui/ .
RUN if [ "$HEADLESS" != "true" ]; then \
      npm run build; \
    else \
      mkdir -p dist && echo "Headless mode - UI skipped" > dist/HEADLESS; \
    fi

# =============================================================================
# Stage 2: Build llama.cpp for CPU (no CUDA)
# =============================================================================
FROM debian:bookworm AS llama-builder
WORKDIR /llama

RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential cmake git \
    && rm -rf /var/lib/apt/lists/*

# Clone and build llama.cpp for CPU
RUN git clone --depth 1 --branch b4782 https://github.com/ggml-org/llama.cpp.git . && \
    cmake -B build \
      -DGGML_BACKEND_DL=ON \
      -DGGML_NATIVE=OFF \
      -DGGML_VULKAN=OFF \
      -DGGML_CUDA=OFF \
      -DGGML_METAL=OFF \
      -DBUILD_SHARED_LIBS=ON \
      -DCMAKE_BUILD_TYPE=Release && \
    cmake --build build -j$(nproc) && \
    mkdir -p /output/lib && \
    cp build/bin/*.so /output/lib/ 2>/dev/null || true && \
    cp build/src/*.so /output/lib/ 2>/dev/null || true && \
    cp build/ggml/src/*.so /output/lib/ 2>/dev/null || true && \
    cp build/ggml/src/ggml-cpu/*.so /output/lib/ 2>/dev/null || true

# =============================================================================
# Stage 3: Go build with Vulkan + localllm (CPU)
# =============================================================================
FROM golang:1.25-bookworm AS builder
ARG HEADLESS=false
WORKDIR /build

# Install Vulkan SDK and development dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential \
    libvulkan1 \
    libvulkan-dev \
    && rm -rf /var/lib/apt/lists/*

# Copy llama.cpp CPU libraries
COPY --from=llama-builder /output/lib /build/lib/llama/

# Go dependencies
COPY go.mod go.sum ./
RUN go mod download

# Source + UI
COPY . .
COPY --from=ui /ui/dist ./ui/dist

# Build with Vulkan + localllm (CPU-based inference for Heimdall)
RUN COMMIT_HASH=$(git rev-parse --short HEAD 2>/dev/null || echo "unknown") && \
    BUILD_TIME=$(date -u +%Y%m%d-%H%M%S) && \
    if [ "$HEADLESS" = "true" ]; then \
      echo "Building headless Vulkan + Heimdall (no UI)..." && \
      CGO_ENABLED=1 go build -tags "localllm noui" \
        -ldflags="-s -w -X main.buildTime=${BUILD_TIME} -X main.commit=${COMMIT_HASH}" \
        -o nornicdb ./cmd/nornicdb; \
    else \
      echo "Building Vulkan + Heimdall with UI..." && \
      CGO_ENABLED=1 go build -tags "localllm" \
        -ldflags="-s -w -X main.buildTime=${BUILD_TIME} -X main.commit=${COMMIT_HASH}" \
        -o nornicdb ./cmd/nornicdb; \
    fi

# Build APOC plugin
RUN echo "Building APOC plugin..." && \
    mkdir -p apoc/built-plugins && \
    cd apoc/plugin-src/apoc && go build -buildmode=plugin -o ../../../apoc/built-plugins/apoc.so apoc_plugin.go && \
    echo "✓ Built plugin:" && ls -lh /build/apoc/built-plugins/*.so

# =============================================================================
# Stage 4: Runtime (Vulkan-enabled with llama.cpp CPU)
# =============================================================================
FROM debian:bookworm-slim
WORKDIR /app

# Install Vulkan runtime libraries and dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    ca-certificates tzdata wget \
    libvulkan1 \
    mesa-vulkan-drivers \
    libgomp1 \
    && rm -rf /var/lib/apt/lists/* && \
    mkdir -p /data /app/models /app/lib/llama

COPY --from=builder /build/nornicdb /app/
COPY --from=builder /build/apoc/built-plugins /app/plugins/
COPY --from=builder /build/lib/llama/*.so /app/lib/llama/
COPY docker/entrypoint.sh /app/
RUN chmod +x /app/entrypoint.sh

# Model embedding: only copy when EMBED_MODEL=true
ARG EMBED_MODEL=false
ARG HEADLESS=false
RUN --mount=type=bind,source=models,target=/models,ro \
    if [ "$EMBED_MODEL" = "true" ]; then \
      if [ -f /models/bge-m3.gguf ]; then \
        cp /models/bge-m3.gguf /app/models/ && \
        echo "✓ Embedded bge-m3.gguf model ($(du -h /app/models/bge-m3.gguf | cut -f1))"; \
      else \
        echo "ERROR: EMBED_MODEL=true but models/bge-m3.gguf not found" && exit 1; \
      fi && \
      if [ -f /models/qwen2.5-0.5b-instruct.gguf ]; then \
        cp /models/qwen2.5-0.5b-instruct.gguf /app/models/ && \
        echo "✓ Embedded Heimdall model ($(du -h /app/models/qwen2.5-0.5b-instruct.gguf | cut -f1))"; \
      fi; \
    else \
      echo "→ BYOM mode (no embedded model)"; \
    fi && \
    if [ "$HEADLESS" = "true" ]; then \
      echo "✓ Headless mode enabled (no UI)"; \
    fi

EXPOSE 7474 7687

HEALTHCHECK --interval=30s --timeout=10s --start-period=10s --retries=3 \
    CMD wget --spider -q http://localhost:7474/health || exit 1

ENV NORNICDB_DATA_DIR=/data \
    NORNICDB_HTTP_PORT=7474 \
    NORNICDB_BOLT_PORT=7687 \
    NORNICDB_EMBEDDING_PROVIDER=local \
    NORNICDB_EMBEDDING_MODEL=bge-m3 \
    NORNICDB_EMBEDDING_DIMENSIONS=1024 \
    NORNICDB_MODELS_DIR=/app/models \
    NORNICDB_NO_AUTH=true \
    NORNICDB_GPU_ENABLED=true \
    NORNICDB_GPU_BACKEND=vulkan \
    NORNICDB_HEIMDALL_ENABLED=true \
    NORNICDB_HEADLESS=${HEADLESS} \
    NORNICDB_PLUGINS_DIR=/app/plugins \
    LD_LIBRARY_PATH=/app/lib/llama

ENTRYPOINT ["/app/entrypoint.sh"]
